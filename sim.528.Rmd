---
title: "Simulation"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = F)
library(mvtnorm)
library(tidyverse)
library(knitr)
```

### Simulation Setup - General

Assume we have measurements of a binary treatment taken at two times, $(A_1, A_2)$, covariates $(X_t, Z_t)$ measured over time (before $A_1$ and before $A_2$), and a final continuous outcome $Y$ measured at the end of the study. We want to assess the effect of the sequential treatment. We assume that the covariates and outcome are correlated due to unmeasured common causes of each, denoted by $u$. However, the causal structure of the variables is unknown. In particular, we do not know whether treatment has a direct effect on the intermediate outcomes. Also unknown is the set of variables that have a direct effect on the outcome. Therefore, we aim to incorporate the uncertainty in the causal structure into our estimation.

### Simulation Setup - Specific

\begin{align}
u_i &\sim N(0,1) \\
x_{1i}, z_{1i} &\sim N(0,1) \\
\text{logit} ( P(a_{1i} = 1 \vert x_{1i}, z_{1i}) ) &= x_{1i}+z_{1i} \\
x_{2i} \vert a_{1i}, x_{1i} &\sim N(0.7x_{1i} - a_{1i} + u_i, 1) \\
z_{2i} \vert a_{1i}, z_{1i} &\sim N(0.8z_{1i} + u_i, 1) \\
\text{logit} ( P(a_{2i} = 1 \vert x_{2i}, z_{2i}) &= x_{2i}+z_{2i} \\
y_{i} \vert \cdot &\sim N(0.7x_{2i} - a_{2i} + u_i, 1)
\end{align}


### Causal Effect Definition

We aim to estimate the effect of treatment strategy $g^\prime = (1,1)$ versus treatment strategy $g=(0,0)$ using the sample average treatment effect,

\begin{align}
\phi(g^\prime, g) = \frac{1}{N} \sum_{i=1}^N \bigg(Y_i^{g^\prime} - Y_i^{g} \bigg).
\end{align}


### Model Assumptions

Each variable that we model is normal and for each we use the following assumptions (simplest case of spike + slab). Under our assumptions, we equate the elements of $\boldsymbol{\gamma}$ with the existence of edges in the causal graph. In the setup described below, we use $\mathbf{y,X}$ to generically deonte a response vector and design matrix. We aim to sample from the joint posterior, $p(\theta, \gamma, \beta, \sigma^2 \vert \textbf{Data})$. We do so under the following prior assumptions,

\begin{align}
\sigma^2 &\sim \text{IG}(1/2,1/2) \\
P(\theta) &= {1}(0<\theta<1) \\
p(\gamma_i=1 \vert \theta) &= \theta \\
p(\beta_i \vert \gamma_i) &= \gamma_i \mathcal{ N}(0,\nu_1) + (1-\gamma_i)\mathcal{N}(0,\nu_0) \\
p(\mathbf{y} \vert \boldsymbol{\beta}, \mathbf{X}) &= \mathcal{N}(\mathbf{X} \beta, \sigma^2 \mathbf{I}) \\
\end{align}

Which allow a simple gibbs sampling algorithm in which the conditional distributions are,

\begin{align}
p(\theta \vert \gamma) &\sim \text{Beta}(\sum_i \gamma_i + 1, p - \sum_i \gamma_i + 1) \\
p(\gamma_i \vert \beta, \theta) &= \frac{\theta p(\beta \vert \gamma=1)}{\theta p(\beta \vert \gamma=1)+(1-\theta) p(\beta \vert \gamma=0)} \\
&= \frac{\theta N(0,\nu_1)}{\theta N(0,\nu_1) + (1-\theta) N(0, \nu_0)} \\
p(\beta \vert \gamma) &\sim N( (\sigma^{-2}) \mathbf{M}^{-1} X^\top y, \mathbf{M}^{-1}) \\
p(\sigma^2 \vert \cdot) &\sim \text{IG}((n+1)/2, (\text{ss}+1)/2)
\end{align}

where $$\mathbf{M} =(\sigma^{-2}X^\top X + D(\gamma)^{-1})$$


```{r simulate_data}
### simulate data 
### 2 covariates, A_0 has a direct effect on one but not the other
### y depends on the last measured treatment and X1

set.seed(3)
n = 1000
u = rnorm(n, mean = 0, sd = 2)
Z1 = rnorm(n)
X1 = rnorm(n)

A1 = rbinom(n, 1, prob = plogis(Z1 + X1)) # higher probability of treatment when X's are >

Z2 = rnorm(n, mean = 0.8 * Z1 + u)
X2 = rnorm(n, mean = 0.7 * X1 - A1 + u) # responds to treatment

A2 = rbinom(n, 1, prob = plogis(Z2 + X2)) # higher probability of treatment when X's are >

Y = rnorm(n, mean = 0.7 * X2 - A2 + u)

dat = data.frame(Z1, Z2, X1, X2, A1, A2, Y)
```

```{r gibbs_fn}
# Gibbs sampler for new parameter values
# input: old parameter values (theta, gamma, beta, sigsq), data (design, response)
# output: list of new sampled values for each parameter

sample.new = function(theta.old, gamma.old, beta.old, sigsq.old, design, resp) {
  
  p = ncol(design)
  # sample new theta
  # assumption: U(0,1) prior on theta/ binomial distribution on gamma (beta - binomial)
  
  theta.new = rbeta(1, shape1 = sum(gamma.old) + 1  ,shape2 = p - sum(gamma.old) + 1)

  # sample new gamma 
  # assumption: independent elements, fixed nu_0, nu_1

  pg = rep(0,p) # hold the gamma probabilities
  
  for(pred in 1:p) {
    a = (theta.new * dnorm(beta.old[pred],mean = 0, sd = nu_1))
    b = (1-theta.new) * dnorm(beta.old[pred],mean = 0, sd = nu_0)
    pg[pred] = a/(a+b)
  }
  
  gamma.new = rbinom(p, 1, prob = pg) # now, sample based on posterior probabilities
  
  # sample new beta
  # assume scale mixture prior and normal likelihood for response with sigma = sigma^2 * I
  M = (t(design) %*% design)*sigsq.old^-1 + diag(ifelse(gamma.new == 1, 1/nu_1, 1/nu_0)) 
  Minv = solve(M)
  meanvec = sigsq.old^-1 * Minv %*% t(design) %*% resp
  beta.new = rmvnorm(1, mean = meanvec, sigma = Minv)
  
  # sample new sigsq
  # assumptions: IG(.5, .5) prior on sigsq
  ss = sum ( (resp - design %*% array(beta.new, dim=p) )^2 )
  n = length(resp)
  sigsq.new = rgamma(1, (n+1)/2, (ss+1)/2)
  
  
  return(list(theta.new=theta.new, 
              gamma.new=gamma.new, 
              beta.new=beta.new,
              sigsq.new=sigsq.new))
  
  
}
```

```{r mcmc_over_structure}

set.seed(2)
nIter = 1000

nu_1 = 5
nu_0 = 0.3

p = 2 # covariate model design matrix dimension
pY = 4 # outcome model design matrix dimension

# array of parameters for each intermediate variable
# theta (1)/ gamma (p)/ beta (p)/ sigsq (1)

params = array(.1, dim = c(2, nIter, 2 * p + 2)) # intermediate variables
paramsY = array(.1, dim = c(nIter, 2 * pY + 2))


for(it in 2:nIter) {
  # sample theta
  
  
  
  ############## sample X's #####################
  new.params = sample.new(theta.old = params[1, it-1, 1],
                          gamma.old = params[1, it-1, 2:(p+1)],
                          beta.old = params[1, it-1, (p+2):(2*p+1)],
                          sigsq.old = params[1, it-1, 2*p+2],
                          design = cbind(X1, A1),
                          resp = X2)
  
  params[1, it, 1] = new.params$theta.new
  params[1, it, 2:(p+1)] = new.params$gamma.new
  params[1, it, (p+2):(2*p+1)] = new.params$beta.new
  params[1, it, 2*p+2] = new.params$sigsq.new
  
  ############## sample Z's #####################
  new.params = sample.new(theta.old = params[2, it-1, 1],
                        gamma.old = params[2, it-1, 2:(p+1)],
                        beta.old = params[2, it-1, (p+2):(2*p+1)],
                        sigsq.old = params[2, it-1, 2*p+2],
                        design = cbind(Z1, A1),
                        resp = Z2)
  
  params[2, it, 1] = new.params$theta.new
  params[2, it, 2:(p+1)] = new.params$gamma.new
  params[2, it, (p+2):(2*p+1)] = new.params$beta.new
  params[2, it, 2*p+2] = new.params$sigsq.new
  
  ############## sample Y's #######################
  pY = 4
  new.params = sample.new(theta.old = paramsY[it-1, 1],
                      gamma.old = paramsY[it-1, 2:(pY+1)],
                      beta.old = paramsY[it-1, (pY+2):(2*pY+1)],
                      sigsq.old = paramsY[it-1, 2*pY+2],
                      design = cbind(X2, Z2, A1, A2),
                      resp = Y)
  
  paramsY[it, 1] = new.params$theta.new
  paramsY[it, 2:(pY+1)] = new.params$gamma.new
  paramsY[it, (pY+2):(2*pY+1)] = new.params$beta.new
  paramsY[it, 2*pY+2] = new.params$sigsq.new
  
}

```

### Posterior probabilities over structures, $P(\mathcal{G} \vert \mathbf{D})$


```{r}
edges = data.frame(cbind(params[1,,2:3], params[2,,2:3], paramsY[,2:5]))
names(edges) = c("X1.X2", "A1.X2", "Z1.Z2", "A1.Z2", "X2.Y", "Z2.Y", "A1.Y", "A2.Y")
edges = edges[(10:nIter),]

a =edges %>% group_by(X1.X2, A1.X2, Z1.Z2, A1.Z2, X2.Y, Z2.Y, A1.Y, A2.Y) %>% 
  summarise(n=n(), .groups = 'drop') %>%
  arrange(desc(n)) %>% ungroup() #%>% mutate(prob = round(n/sum(n), 2))  %>% select(!n)

a$prob = round(a$n/(nrow(edges)), 2)

kable(a[1:10, -9])

#, A1-X2, Z1-Z2, A1-Z2, X2-Y, Z2-Y, A1-Y, A2-Y
#library(rstanarm)
#ymod = stan_glm(Y ~ A2 + X2, family = gaussian(), data = dat)
#yparams = as.matrix(ymod)

```

Rows of this table represent causal graphs, used as input for the g-gformula to estimate $\phi$. Each column corresponds with an edge where the first variable is the source node. The first column corresponds with the existence of the $X1 \rightarrow X2$ edge.

### Posterior distribution of $\phi$

```{r}
# completed dataset for each set of parameters
# RANDOM G
# augmented data set


dat$y11 = NA
dat$y00 = NA
dat$t1 = ifelse(dat$A1 == 1 & dat$A2 == 1, 1, 0)
dat$t0 = ifelse(dat$A1 == 0 & dat$A2 == 0, 1, 0)

# first, let's do this with the correct G
row = 100
ndraws = 500

diffs = array(0, dim = ndraws)

for(draw in 1:ndraws) {
  
  if ( params[1, row + draw, 3] == 1 ) { # if there is an edge from treatment to covariate, impute
  
    mx1 = cbind(dat$X1, 1) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    X2draw1 = rnorm(n, mean = mx1, sd = params[1, row + draw, 6]) # fixed treatment to 1
    X2draw1 = ifelse(dat$A1 == 1, dat$X2, X2draw1)
    
    
    mx0 = cbind(dat$X1, 0) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    X2draw0 = rnorm(n, mean = mx0, sd = params[1, row + draw, 6]) # fixed treatment to 1
    X2draw0 = ifelse(dat$A1 == 0, dat$X2, X2draw0)

  
  } else { 
    
    X2draw1 = dat$X2
    X2draw0 = dat$X2
    
  }
  
  if ( params[2, row + draw, 3] == 1 ) { # if there is an edge from treatment to covariate, impute
  
    mz1 = cbind(dat$Z1, 1) %*% params[2, row + draw, c(4,5)]  # XB (treated at time 1)
    Z2draw1 = rnorm(n, mean = mz1, sd = params[2, row + draw, 6]) # fixed treatment to 1
    Z2draw1 = ifelse(dat$A1 == 1, dat$Z2, Z2draw1)
    
    
    mz0 = cbind(dat$Z1, 0) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    Z2draw0 = rnorm(n, mean = mz0, sd = params[1, row + draw, 6]) # fixed treatment to 1
    Z2draw0 = ifelse(dat$A1 == 0, dat$Z2, Z2draw0)

  
  } else { 
    
    Z2draw1 = dat$Z2
    Z2draw0 = dat$Z2
    
    }

  
  
  
  
  my1 = cbind(X2draw1, Z2draw1, 1, 1) %*% paramsY[(row+draw),c(6:9)] # always treated
  Ydraw1 = rnorm(n, mean = my1, sd = paramsY[(row+draw), 10])
  
  my0 = cbind(X2draw0, Z2draw0, 1, 1) %*% paramsY[(row+draw),c(6:9)] # always treated
  Ydraw0 = rnorm(n, mean = my0, sd = paramsY[(row+draw), 10])

  
  dat$y11 = ifelse(dat$t1 == 1, dat$Y, Ydraw1)
  dat$y00 = ifelse(dat$t0 == 1, dat$Y, Ydraw0)
  
  diffs[draw] = sum(dat$y11 - dat$y00)/n
  
  
}

#hist(diffs)
d.random = data.frame(diffs = diffs, g = "ran" )

```


```{r}

# augmented dataset
dat$y11 = NA
dat$y00 = NA
dat$t1 = ifelse(dat$A1 == 1 & dat$A2 == 1, 1, 0)
dat$t0 = ifelse(dat$A1 == 0 & dat$A2 == 0, 1, 0)

# first, let's do this with the correct G
row = 100
ndraws = 500

diffs = array(0, dim = ndraws)


for(draw in 1:ndraws) {
  
  if ( TRUE ) { # if there is an edge from treatment to covariate, impute/params[1, row + draw, 3] == 1
  
    mx1 = cbind(dat$X1, 1) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    X2draw1 = rnorm(n, mean = mx1, sd = params[1, row + draw, 6]) # fixed treatment to 1
    
    mx0 = cbind(dat$X1, 0) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    X2draw0 = rnorm(n, mean = mx0, sd = params[1, row + draw, 6]) # fixed treatment to 1

  } else { X2draw = dat$X2 }
  
  X2draw1 = ifelse(dat$A1 == 1, dat$X2, X2draw1)
  X2draw0 = ifelse(dat$A1 == 0, dat$X2, X2draw0)
  
  my1 = cbind(X2draw1, dat$Z2, 1, 1) %*% paramsY[(row+draw),c(6:9)] # always treated
  Ydraw1 = rnorm(n, mean = my1, sd = paramsY[(row+draw), 10])
  
  my0 = cbind(X2draw0, dat$Z2, 1, 1) %*% paramsY[(row+draw),c(6:9)] # always treated
  Ydraw0 = rnorm(n, mean = my0, sd = paramsY[(row+draw), 10])

  
  dat$y11 = ifelse(dat$t1 == 1, dat$Y, Ydraw1)
  dat$y00 = ifelse(dat$t0 == 1, dat$Y, Ydraw0)
  
  diffs[draw] = sum(dat$y11 - dat$y00)/n
  
  
}


#hist(diffs)

d.fixed = data.frame(diffs, g = "fixed")
d.ran2 = rbind(d.random, d.fixed)

ggplot(d.ran2, aes(x = diffs, fill = g)) + geom_density(aes(color = g, alpha = 0.5)) + theme_minimal()


```

This plot shows the distribution of $\phi$ when a a fixed graph is used (red) and when the graph is sampled (green). Since $\phi$ is sensitive to the assumed causal graph, this distribution is multi-modal.


### Open questions
- multiple time points "rules"
- good choice of hyperparameters ($\nu_0, \nu_1$)



