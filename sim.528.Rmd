---
title: "sim.5.28"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
```

### sample difference


### estimand

maybe do a sample level predictor

## One outcome/ One predictor

Assume we have $p$ covariates, $(X_{jt}$ $j=1,\ldots,p$, $t=0,1)$ , measured before $A_0$ ($t=0$) and also before $A_1$ ($t=1$). Suppose that treatment decisions depend on all covariates and direct effects of treatment on each covariate are unknown. Each element of $X_{jt}$ is sampled from a normal distribution.

\begin{align}
\sigma^2 &\sim \text{IG}(1/2,1/2) \\
P(\theta) &= {1}(0<\theta<1) \\
p(\gamma_i=1 \vert \theta) &= \theta \\
p(\beta_i \vert \gamma_i) &= \gamma_i \mathcal{ N}(0,\nu_1) + (1-\gamma_i)\mathcal{N}(0,\nu_0) \\
p(\mathbf{x_{j1}} \vert \boldsymbol{\beta}, \mathbf{X}) &= \mathcal{N}(\mathbf{X} \beta, \sigma^2 \mathbf{I}) \\
\end{align}

The design matrix $\mathbf{X}$ consists of the previous measurement of $\mathbf{x}_{i1}$ as well as the previous treatment, $\mathbf{A}_0$.

$$p(\theta, \gamma, \beta, \sigma^2 \vert \textbf{Data})$$

\begin{align}
p(\theta \vert \gamma) &\sim \text{Beta}(\sum_i \gamma_i + 1, p - \sum_i \gamma_i + 1) \\
p(\gamma_i \vert \beta, \theta) &= \frac{\theta p(\beta \vert \gamma=1)}{\theta p(\beta \vert \gamma=1)+(1-\theta) p(\beta \vert \gamma=0)} \\
&= \frac{\theta N(0,\nu_1)}{\theta N(0,\nu_1) + (1-\theta) N(0, \nu_0)} \\
p(\beta \vert \gamma) &\sim N( (\sigma^{-2}) \mathbf{M}^{-1} X^\top y, \mathbf{M}^{-1}) \\
p(\sigma^2 \vert \cdot) &\sim \text{IG}((n+1)/2, (\text{ss}+1)/2)
\end{align}

where $$\mathbf{M} =(\sigma^{-2}X^\top X + D(\gamma)^{-1})$$


```{r simulate data}
### simulate data 
### 2 covariates, A_0 has a direct effect on one but not the other
### y depends on the last measured treatment and X1

set.seed(3)
n = 1000
u = rnorm(n, mean = 0, sd = 2)
Z1 = rnorm(n)
X1 = rnorm(n)

A1 = rbinom(n, 1, prob = plogis(Z1 + X1)) # higher probability of treatment when X's are >

Z2 = rnorm(n, mean = 0.8 * Z1 + u)
X2 = rnorm(n, mean = 0.7 * X1 - A1 + u) # responds to treatment

A2 = rbinom(n, 1, prob = plogis(Z2 + X2)) # higher probability of treatment when X's are >

Y = rnorm(n, mean = 0.7 * X2 - A2 + u)

dat = data.frame(Z1, Z2, X1, X2, A1, A2, Y)
```

```{r gibbs sampling fn}
# Gibbs sampler for new parameter values
# input: old parameter values (theta, gamma, beta, sigsq), data (design, response)
# output: list of new sampled values for each parameter

sample.new = function(theta.old, gamma.old, beta.old, sigsq.old, design, resp) {
  
    p = ncol(design)
  
  # sample new theta
  # assumpion: U(0,1) prior on theta/ binomial distribution on gamma (beta - binomial)
  
  theta.new = rbeta(1, shape1 = sum(gamma.old) + 1  ,shape2 = p - sum(gamma.old) + 1)

  # sample new gamma 
  # assumption: independent elements, fixed nu_0, nu_1

  pg = rep(0,p) # hold the gamma probabilities
  
  for(pred in 1:p) {
    a = (theta.new * dnorm(beta.old[pred],mean = 0, sd = nu_1))
    b = (1-theta.new) * dnorm(beta.old[pred],mean = 0, sd = nu_0)
    pg[pred] = a/(a+b)
  }
  
  gamma.new = rbinom(p, 1, prob = pg) # now, sample based on posterior probabilities
  
  # sample new beta
  # assume scale mixture prior and normal likelihood for response with sigma = sigma^2 * I
  M = (t(design) %*% design)*sigsq.old^-1 + diag(ifelse(gamma.new == 1, 1/nu_1, 1/nu_0)) 
  Minv = solve(M)
  meanvec = sigsq.old^-1 * Minv %*% t(design) %*% resp
  beta.new = rmvnorm(1, mean = meanvec, sigma = Minv)
  
  # sample new sigsq
  # assumptions: IG(.5, .5) prior on sigsq
  ss = sum ( (resp - design %*% array(beta.new, dim=p) )^2 )
  n = length(resp)
  sigsq.new = rgamma(1, (n+1)/2, (ss+1)/2)
  
  
  return(list(theta.new=theta.new, 
              gamma.new=gamma.new, 
              beta.new=beta.new,
              sigsq.new=sigsq.new))
  
  
}
```

```{r mcmc over structure}

set.seed(2)
nIter = 1000

nu_1 = 5
nu_0 = 0.2

p = 2
pY = 4

# array of parameters for each intermediate variable
# theta (1)/ gamma (p)/ beta (p)/ sigsq (1)

params = array(.1, dim = c(2, nIter, 2 * p + 2)) # intermediate variables
paramsY = array(.1, dim = c(nIter, 2 * pY + 2))


for(it in 2:nIter) {
  # sample theta
  
  
  
  ############## sample X's #####################
  new.params = sample.new(theta.old = params[1, it-1, 1],
                          gamma.old = params[1, it-1, 2:(p+1)],
                          beta.old = params[1, it-1, (p+2):(2*p+1)],
                          sigsq.old = params[1, it-1, 2*p+2],
                          design = cbind(X1, A1),
                          resp = X2)
  
  params[1, it, 1] = new.params$theta.new
  params[1, it, 2:(p+1)] = new.params$gamma.new
  params[1, it, (p+2):(2*p+1)] = new.params$beta.new
  params[1, it, 2*p+2] = new.params$sigsq.new
  
  ############## sample Z's #####################
  new.params = sample.new(theta.old = params[2, it-1, 1],
                        gamma.old = params[2, it-1, 2:(p+1)],
                        beta.old = params[2, it-1, (p+2):(2*p+1)],
                        sigsq.old = params[2, it-1, 2*p+2],
                        design = cbind(Z1, A1),
                        resp = Z2)
  
  params[2, it, 1] = new.params$theta.new
  params[2, it, 2:(p+1)] = new.params$gamma.new
  params[2, it, (p+2):(2*p+1)] = new.params$beta.new
  params[2, it, 2*p+2] = new.params$sigsq.new
  
  ############## sample Y's #######################
  pY = 4
  new.params = sample.new(theta.old = paramsY[it-1, 1],
                      gamma.old = paramsY[it-1, 2:(pY+1)],
                      beta.old = paramsY[it-1, (pY+2):(2*pY+1)],
                      sigsq.old = paramsY[it-1, 2*pY+2],
                      design = cbind(X2, Z2, A1, A2),
                      resp = Y)
  
  paramsY[it, 1] = new.params$theta.new
  paramsY[it, 2:(pY+1)] = new.params$gamma.new
  paramsY[it, (pY+2):(2*pY+1)] = new.params$beta.new
  paramsY[it, 2*pY+2] = new.params$sigsq.new
  
}

#library(rstanarm)
#ymod = stan_glm(Y ~ A2 + X2, family = gaussian(), data = dat)
#yparams = as.matrix(ymod)

```


```{r posterior predict ran}

# TODO

# completed data set based on a draw from the posterior

# completed dataset for each set of parameters

# augmented dataset
dat$y11 = NA
dat$y00 = NA
dat$t1 = ifelse(dat$A1 == 1 & dat$A2 == 1, 1, 0)
dat$t0 = ifelse(dat$A1 == 0 & dat$A2 == 0, 1, 0)

# first, let's do this with the correct G
row = 100
ndraws = 500

diffs = array(0, dim = ndraws)


for(draw in 1:ndraws) {
  
  if ( TRUE ) { # if there is an edge from treatment to covariate, impute/params[1, row + draw, 3] == 1
  
    mx1 = cbind(dat$X1, 1) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    X2draw1 = rnorm(n, mean = mx1, sd = params[1, row + draw, 6]) # fixed treatment to 1
    
    mx0 = cbind(dat$X1, 0) %*% params[1, row + draw, c(4,5)]  # XB (treated at time 1)
    X2draw0 = rnorm(n, mean = mx0, sd = params[1, row + draw, 6]) # fixed treatment to 1

  
  } else { X2draw = dat$X2 }
  
  X2draw1 = ifelse(dat$A1 == 1, dat$X2, X2draw1)
  X2draw0 = ifelse(dat$A1 == 0, dat$X2, X2draw0)
  
  my1 = cbind(X2draw1, dat$Z2, 1, 1) %*% paramsY[(row+draw),c(6:9)] # always treated
  Ydraw1 = rnorm(n, mean = my1, sd = paramsY[(row+draw), 10])
  
  my0 = cbind(X2draw0, dat$Z2, 1, 1) %*% paramsY[(row+draw),c(6:9)] # always treated
  Ydraw0 = rnorm(n, mean = my0, sd = paramsY[(row+draw), 10])

  
  dat$y11 = ifelse(dat$t1 == 1, dat$Y, Ydraw1)
  dat$y00 = ifelse(dat$t0 == 1, dat$Y, Ydraw0)
  
  diffs[draw] = sum(dat$y11 - dat$y00)/n
  
  
}


hist(diffs)

#ggplot(Ydraw, aes(x = Y, group = nrep)) + geom_density(color = "blue") + 
#  theme_minimal() 
```






```{r posterior predict fixed}

# TODO
# y_11 based on the correct graph - check
# then sample the graph as well

# draw 1: row 100 of params



row = 100
ndraws = 10
ppY = (array(0, dim = c(ndraws, n)))
#ppY$nrep = 1:ndraws
#j = ppY %>% pivot_longer(!it)
 
for(draw in 1:ndraws) {
  
  mx = cbind(dat$X2, 1) %*% params[1, row + draw, c(4,5)]  # XB
  X2draw = rnorm(n, mean = mx, sd = params[1, row, 6]) # fixed treatment to 1
  
  my = cbind(1, 1, X2draw) %*% yparams[(row+draw),1:3]
  Ydraw = rnorm(n, mean = my, sd = yparams[(row+draw), 4])
  ppY[draw,] = Ydraw
  
}



Ydraw = data.frame(Y = array(t(ppY)),
                     nrep = rep(1:ndraws, each = n) )

ggplot(Ydraw, aes(x = Y, group = nrep)) + geom_density(aes(color = "blue")) + 
  theme_minimal() 

# now, let the graph be sampled


#ggplot(plot.df, aes(x = Y, group = nrep)) + geom_density(aes(color=int)) + 
#  theme_minimal() + theme(legend.position = "none") + scale_colour_manual(values=cbp1) + 
#  ggtitle(TeX("Distribution of $Y^{00}$ (Blue) and $Y^{11}$ (Green) under $G_1$")) +
#  ylab(TeX("p($y^g | Data, G_1$)")) + xlim(-15,15) + 
#  geom_vline(xintercept = c(ey00,ey11), color = cbp1[1:2])

#ifelse(params[1, row, 3] == 1, draws, X)


AtoX =  # direct effect from treatment
if(AtoX == 1) {
  # if the edge exists, then sample a (counterfactual) covariate from posterior predictive X2

  
}


```





```{r}
gams = data.frame(params[1,-1,2:3])
gams %>% group_by(X1, X2) %>% summarise(n=n()) %>% arrange(desc(n))
```


```{r}

# 1. likely structures
# 2. 

# tweak this to take multiple predictors (or one)

# simulate data
set.seed(1)
n = 500
# binary predictor
X = cbind( rbinom(n, 1, prob = 0.4), rnorm(n) )
beta.true = c(0.2, 1)
p = length(beta.true)
y = rnorm(n, mean = X %*% beta.true)

# MCMC

nIter = 1000

nu_1 = 5
nu_0 = 0.4

theta.samples = array(0.5, dim = nIter)
gamma.samples = array(0, dim = c(nIter,p))
beta.samples = array(0, dim = c(nIter,p))
sigsq.samples = array(1, dim = nIter)

for(it in 2:nIter) {
  # sample theta
  
  new.params = sample.new(theta.samples[it-1],
                          gamma.samples[it-1,],
                          beta.samples[it-1,],
                          sigsq.samples[it-1],
                          design = X,
                          resp = y)
  
  theta.samples[it] = new.params$theta.new
  gamma.samples[it,] = new.params$gamma.new
  beta.samples[it,] = new.params$beta.new
  sigsq.samples[it] = new.params$sigsq.new
  
}


# for(it in 2:nIter) {
#   # sample theta
#   theta.samples[it] = rbeta(1, shape1 = sum(gamma.samples[it-1,]) + 1  ,shape2 = p - sum(gamma.samples[it-1,]) + 1 )
#   
#   # sample gamma
#   pg = rep(0,p) # hold the gamma probabilities
#   
#   for(pred in 1:p) {
#     
#     a = (theta.samples[it] * dnorm(beta.samples[it-1,p],mean = 0, sd = nu_1))
#     b = (1-theta.samples[it]) * dnorm(beta.samples[it-1,p],mean = 0, sd = nu_0)
#     pg[pred] = a/(a+b)
#     
#   }
#   
#   gamma.samples[it,] = rbinom(p, 1, prob = pg)
#   
#   # sample beta
#   M =  t(X) %*% X * ( sigsq.samples[it-1]^(-1) ) + diag ( ifelse(gamma.samples[it,] == 1, 1/nu_1, 1/nu_0)) 
#   beta.samples[it,] = rmvnorm(1, mean = sigsq.samples[it-1]^(-1) * solve(M) %*% t(X) %*% y, sigma = solve(M) )
#   
#   # sample sigsq
#   ss = sum ( (y - X %*% beta.samples[it,])^2 )
#   sigsq.samples[it] = 1/rgamma(1, shape = (n+1)/2, rate = (ss + 1)/2  )
#   
# }

summary(lm(y~X))

hist(beta.samples[2:nIter,2])
hist(gamma.samples[,1])
```

open questions
- multiple time points "rules"
- good choice of hyperparameters



